{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd97963d-c68a-4811-8abf-355674a6e54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b720673-6ef9-4d1b-ab80-78959e0f538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./enron_spam_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6aaf4754-c10a-4022-b2d2-26d69b71d311",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message ID</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Message</th>\n",
       "      <th>Spam/Ham</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>christmas tree farm pictures</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ham</td>\n",
       "      <td>1999-12-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>vastar resources , inc .</td>\n",
       "      <td>gary , production from the high island larger ...</td>\n",
       "      <td>ham</td>\n",
       "      <td>1999-12-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>calpine daily gas nomination</td>\n",
       "      <td>- calpine daily gas nomination 1 . doc</td>\n",
       "      <td>ham</td>\n",
       "      <td>1999-12-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>re : issue</td>\n",
       "      <td>fyi - see note below - already done .\\nstella\\...</td>\n",
       "      <td>ham</td>\n",
       "      <td>1999-12-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>meter 7268 nov allocation</td>\n",
       "      <td>fyi .\\n- - - - - - - - - - - - - - - - - - - -...</td>\n",
       "      <td>ham</td>\n",
       "      <td>1999-12-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33711</th>\n",
       "      <td>33711</td>\n",
       "      <td>= ? iso - 8859 - 1 ? q ? good _ news _ c = eda...</td>\n",
       "      <td>hello , welcome to gigapharm onlinne shop .\\np...</td>\n",
       "      <td>spam</td>\n",
       "      <td>2005-07-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33712</th>\n",
       "      <td>33712</td>\n",
       "      <td>all prescript medicines are on special . to be...</td>\n",
       "      <td>i got it earlier than expected and it was wrap...</td>\n",
       "      <td>spam</td>\n",
       "      <td>2005-07-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33713</th>\n",
       "      <td>33713</td>\n",
       "      <td>the next generation online pharmacy .</td>\n",
       "      <td>are you ready to rock on ? let the man in you ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>2005-07-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33714</th>\n",
       "      <td>33714</td>\n",
       "      <td>bloow in 5 - 10 times the time</td>\n",
       "      <td>learn how to last 5 - 10 times longer in\\nbed ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>2005-07-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33715</th>\n",
       "      <td>33715</td>\n",
       "      <td>dear sir , i am interested in it</td>\n",
       "      <td>hi : )\\ndo you need some softwares ? i can giv...</td>\n",
       "      <td>spam</td>\n",
       "      <td>2005-07-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33716 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Message ID                                            Subject  \\\n",
       "0               0                       christmas tree farm pictures   \n",
       "1               1                           vastar resources , inc .   \n",
       "2               2                       calpine daily gas nomination   \n",
       "3               3                                         re : issue   \n",
       "4               4                          meter 7268 nov allocation   \n",
       "...           ...                                                ...   \n",
       "33711       33711  = ? iso - 8859 - 1 ? q ? good _ news _ c = eda...   \n",
       "33712       33712  all prescript medicines are on special . to be...   \n",
       "33713       33713              the next generation online pharmacy .   \n",
       "33714       33714                     bloow in 5 - 10 times the time   \n",
       "33715       33715                   dear sir , i am interested in it   \n",
       "\n",
       "                                                 Message Spam/Ham        Date  \n",
       "0                                                    NaN      ham  1999-12-10  \n",
       "1      gary , production from the high island larger ...      ham  1999-12-13  \n",
       "2                 - calpine daily gas nomination 1 . doc      ham  1999-12-14  \n",
       "3      fyi - see note below - already done .\\nstella\\...      ham  1999-12-14  \n",
       "4      fyi .\\n- - - - - - - - - - - - - - - - - - - -...      ham  1999-12-14  \n",
       "...                                                  ...      ...         ...  \n",
       "33711  hello , welcome to gigapharm onlinne shop .\\np...     spam  2005-07-29  \n",
       "33712  i got it earlier than expected and it was wrap...     spam  2005-07-29  \n",
       "33713  are you ready to rock on ? let the man in you ...     spam  2005-07-30  \n",
       "33714  learn how to last 5 - 10 times longer in\\nbed ...     spam  2005-07-30  \n",
       "33715  hi : )\\ndo you need some softwares ? i can giv...     spam  2005-07-31  \n",
       "\n",
       "[33716 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f8dd76b-47c5-4adb-872e-904526a999d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Spam/Ham\n",
       "spam    17171\n",
       "ham     16545\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Spam/Ham'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78537e24-07f9-4811-b511-bbf1c4304a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                         \n",
       "1        gary , production from the high island larger ...\n",
       "2                   - calpine daily gas nomination 1 . doc\n",
       "3        fyi - see note below - already done .\\nstella\\...\n",
       "4        fyi .\\n- - - - - - - - - - - - - - - - - - - -...\n",
       "                               ...                        \n",
       "33711    hello , welcome to gigapharm onlinne shop .\\np...\n",
       "33712    i got it earlier than expected and it was wrap...\n",
       "33713    are you ready to rock on ? let the man in you ...\n",
       "33714    learn how to last 5 - 10 times longer in\\nbed ...\n",
       "33715    hi : )\\ndo you need some softwares ? i can giv...\n",
       "Name: Message, Length: 33716, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Message'].fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a83ab8f6-331f-44c9-b417-44beb39dacf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['combined'] = df['Subject'].fillna(' ') + df['Message'].fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c237e28c-a14d-49bd-9202-12f4b9fc9dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50773497"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(' '.join(df['combined']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2543fdfb-3019-4c2c-8680-4a6d614177bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# too big! need to sample...\n",
    "\n",
    "# first randomize the order\n",
    "df_sample = df.sample(frac=0.01, ignore_index=True).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84be0381-ed4f-4614-a89d-ab7c2a2b2ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420408"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(' '.join(df_sample['combined']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26decb1e-7b34-4fa0-8555-4cea2446af13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39756c2f-cf64-4f6a-bc9b-0e2fab458c63",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2395caeb-a220-4cc7-b642-ea5aa164b788",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 96018\n",
      "Unique tokens: 10403\n",
      "Most common words: [('\\n', 7000), ('.', 4803), ('-', 3946), (',', 3146), ('the', 2376), (':', 1867), ('to', 1820), ('/', 1431), ('and', 1299), ('of', 1117)]\n",
      "Most common nouns: [('com', 206), ('enron', 199), ('subject', 143), ('gas', 124), ('pm', 113), ('|', 112), ('time', 106), ('message', 106), ('company', 100), ('information', 97)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 1*10**6\n",
    "\n",
    "text = ' '.join(df_sample['combined'])\n",
    "doc = nlp(text)\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"Total tokens:\", len(tokens))\n",
    "print(\"Unique tokens:\", len(set(tokens)))\n",
    "\n",
    "word_counts = Counter(tokens)\n",
    "print(\"Most common words:\", word_counts.most_common(10))\n",
    "\n",
    "noun_counts = Counter(token.text for token in doc if token.pos_ == \"NOUN\")\n",
    "print(\"Most common nouns:\", noun_counts.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f839e71f-f9ce-4ebb-891d-16d89e1f4d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bb41476-d2b3-4a47-85e7-8563346729f4",
   "metadata": {},
   "source": [
    "## Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "49d16352-05fd-4fcb-a3b3-90a17a2f230d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/amarks-b/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89f1691-63de-475c-8221-952fe6c7d3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "  \n",
    "documents = df_sample['combined'].values\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n",
    "dictionary = corpora.Dictionary(tokenized_docs)\n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_docs]\n",
    "lda_model = LdaModel(corpus, num_topics=2, id2word=dictionary, passes=10)\n",
    "for topic in lda_model.print_topics():\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46253777-f4d5-44fc-884a-bd3f4add1c9d",
   "metadata": {},
   "source": [
    "## word2vec (in gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaef7bf-6f75-4153-b515-32cb9f8d1712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download punkt tokenizer if you haven't already\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# Your training data (list of sentences/documents)\n",
    "data = df_sample['combined'].values\n",
    "\n",
    "# 1. Tokenize the data (split sentences into words)\n",
    "tokenized_data = [word_tokenize(sentence.lower()) for sentence in data]\n",
    "\n",
    "# 2. Train the Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)\n",
    "# - sentences: The tokenized training data.\n",
    "# - vector_size: Dimensionality of the word vectors (e.g., 100 features per word).\n",
    "# - window: Maximum distance between the current and predicted word within a sentence.\n",
    "# - min_count: Ignores all words with total frequency lower than this.\n",
    "# - workers: Number of worker threads to train the model (for faster training).\n",
    "\n",
    "# 3. Access word vectors (example)\n",
    "word = \"word2vec\"\n",
    "if word in model.wv:\n",
    "    vector = model.wv[word]\n",
    "    print(f\"Vector for '{word}': {vector}\")\n",
    "else:\n",
    "    print(f\"Word '{word}' not found in the vocabulary.\")\n",
    "\n",
    "# 4. Find similar words (example)\n",
    "similar_words = model.wv.most_similar(\"sentence\", topn=3)\n",
    "print(f\"\\nWords similar to 'sentence': {similar_words}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 5. Save the trained model (optional)\n",
    "# model.save(\"word2vec_model.bin\")\n",
    "# To load the model later:\n",
    "# from gensim.models import Word2Vec\n",
    "# loaded_model = Word2Vec.load(\"word2vec_model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be636e2-930f-49b6-9cbd-1be9cc82affb",
   "metadata": {},
   "source": [
    "## Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca0cc11-4096-4b1b-8c2b-ef7bf2b6f21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "documents = [\"This is the first document.\", \"This document is the second document.\", \"And this is the third one.\", \"Is this the first document?\"]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "print(tfidf_matrix.toarray())\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b77f3-b1da-4361-b3f8-4b1f3c688ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74064abd-59b2-4060-9ced-51e8b8eb32a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65ee7a73-bb62-4f12-b4dc-18c86dbb2162",
   "metadata": {},
   "source": [
    "## Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbb40ea-687f-4d2d-808d-5c539cd18382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "result = sentiment_pipeline(\"This is a fantastic movie!\")\n",
    "print(f\"Sentiment: {result}\")\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "result = question_answerer(\n",
    "    question=\"What is my name?\",\n",
    "    context=\"My name is Sarah and I live in London.\"\n",
    ")\n",
    "print(f\"Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accaa811-9804-4964-93c5-626816a58255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119b2d4d-3371-4f00-98ed-b16bceb3ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word2vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_day2",
   "language": "python",
   "name": "venv_day2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
